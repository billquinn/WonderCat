{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WonderCat Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, warnings, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and Store Data\n",
    "\n",
    "### WonderCat API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fetch WonderCat Data through API\n",
    "api_prefix = 'https://env-1120817.us.reclaim.cloud/wp-json/wp/v2/user-experience'\n",
    "\n",
    "def get_total_pagecount():\n",
    "    api_url = f'{api_prefix}?page=1&per_page=100'\n",
    "    response = requests.get(api_url)\n",
    "    pages_count = response.headers['X-WP-TotalPages']\n",
    "    return int(pages_count)\n",
    "\n",
    "def read_wordpress_post_with_pagination():\n",
    "    total_pages = get_total_pagecount()\n",
    "    current_page = 1\n",
    "    all_page_items_json = []\n",
    "    while current_page <= total_pages:\n",
    "        api_url = f\"{api_prefix}?page={current_page}&per_page=100\"\n",
    "        page_items = requests.get(api_url)\n",
    "        page_items_json = page_items.json()\n",
    "        all_page_items_json.extend(page_items_json)\n",
    "        current_page = current_page + 1\n",
    "    return all_page_items_json\n",
    "\n",
    "# Transform API JSON to Dataframe\n",
    "def transform_to_dataframe(api_call):\n",
    "    api_data = pd.DataFrame(api_call)\n",
    "    api_data = api_data[['id', 'author', 'date', 'benefit', 'experience', 'technology', 'acf']]\n",
    "    # This should be cleaner...\n",
    "    api_data['bene_del'] = pd.json_normalize(api_data['benefit'])\n",
    "    api_data['benefit'] = pd.json_normalize(api_data['bene_del'])['name']\n",
    "    api_data['exp_del'] = pd.json_normalize(api_data['experience'])\n",
    "    api_data['experience'] = pd.json_normalize(api_data['exp_del'])['name']\n",
    "    api_data['tech_del'] = pd.json_normalize(api_data['technology'])\n",
    "    api_data['technology'] = pd.json_normalize(api_data['tech_del'])['name']\n",
    "    api_data['text'] = pd.json_normalize(api_data['acf'])['feature']\n",
    "    api_data['QID'] = pd.json_normalize(api_data['acf'])['wikidata-qid']\n",
    "    del api_data['acf'], api_data['bene_del'], api_data['exp_del'], api_data['tech_del']\n",
    "\n",
    "    # Convert date of experience to Y-m-d\n",
    "    api_data['date'] = api_data['date'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', '\\\\1', regex = True)\n",
    "    api_data['date'] = pd.to_datetime(api_data['date'])\n",
    "\n",
    "    return api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather all QID's from dataframe.\n",
    "def get_QIDS(df):\n",
    "    # Gather QIDS and validate with regular expression.\n",
    "    QIDS = df['QID'].unique()\n",
    "    regex = re.compile(r'Q\\d+')\n",
    "    QIDS = [s for s in QIDS if regex.match(s)]\n",
    "\n",
    "    # Append 'wd:' prefix for sparql query.\n",
    "    QIDS = ' '.join(['wd:' + x for x in QIDS if isinstance(x, str)])\n",
    "\n",
    "    return QIDS\n",
    "\n",
    "\n",
    "# Build SPARQL query.\n",
    "def build_query_call_api(QIDS):\n",
    "    QIDS = QIDS\n",
    "\n",
    "    # Build SPARQL Query.\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        ?item ?itemLabel\n",
    "        (group_concat(DISTINCT(?dateLabel); separator=',') as ?pubDates)\n",
    "        (group_concat(DISTINCT(?genreLabel); separator=',') as ?genres)\n",
    "        (group_concat(DISTINCT(?countryOriginLabel); separator=',') as ?origin)\n",
    "        (group_concat(DISTINCT(?coordinatesLabel); separator=',') as ?coordinates)\n",
    "\n",
    "        WHERE {\n",
    "            VALUES ?item { %s }\n",
    "            ?item wdt:P31 ?instanceof.\n",
    "            OPTIONAL{?item wdt:P577 ?pubDate}.\n",
    "            OPTIONAL{?item wdt:P136 ?genre}.\n",
    "            ?item wdt:P495 ?origin.\n",
    "            ?origin wdt:P625 ?coordinates.\n",
    "\n",
    "            SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language 'en,en'.\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            ?pubDate rdfs:label ?dateLabel.\n",
    "            ?genre rdfs:label ?genreLabel.\n",
    "            ?origin rdfs:label ?countryOriginLabel.\n",
    "            ?coordinates rdfs:label ?coordinatesLabel.\n",
    "            }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (QIDS)\n",
    "\n",
    "    # Call API\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    res = requests.get(url, params={'query': sparql_query, 'format': 'json'}).json()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create dataframe from API results.\n",
    "def api_to_dataframe(res):\n",
    "    wiki_df =[]\n",
    "\n",
    "    # Loop through WikiQuery Results.\n",
    "    for i in res['results']['bindings']:\n",
    "        # Build empty dictionary.\n",
    "        wiki_item = {}\n",
    "        # Loop through each item's keys.\n",
    "        for k in i.keys():\n",
    "            # Append values to wiki_item\n",
    "            wiki_item[k] = i[k]['value']\n",
    "\n",
    "        # Once item's keys looped, append new dictionary to list for dataframe.\n",
    "        wiki_df.append(wiki_item)\n",
    "\n",
    "    wiki_df = pd.DataFrame(wiki_df)\n",
    "\n",
    "    # Clean up item/QID field.\n",
    "    wiki_df['item'] = wiki_df['item'].str.replace(r'.*/(Q\\d+)', '\\\\1', regex = True)\n",
    "    wiki_df = wiki_df.rename(columns = {'item':'QID'})\n",
    "\n",
    "    # Clean up date field. Currently returning only year due to some dates being \"out of bounds\" (too old).\n",
    "    wiki_df['pubDates'] = wiki_df['pubDates'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', r'\\\\1', regex = True)\n",
    "    wiki_df['pubDates'] = pd.to_datetime(wiki_df['pubDates'], errors = 'coerce')\n",
    "\n",
    "    # Create Longitude and Latitude columns.\n",
    "    reg_pattern = r'Point\\(([-]?\\d+\\.?\\d+)\\s([-]?\\d+\\.?\\d+)\\)'\n",
    "    wiki_df['lon'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\1', regex = True)\n",
    "    wiki_df['lat'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\2', regex = True)\n",
    "\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write WonderCat API Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 411 ms, total: 2.69 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>benefit</th>\n",
       "      <th>experience</th>\n",
       "      <th>technology</th>\n",
       "      <th>text</th>\n",
       "      <th>QID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>916</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Confusion</td>\n",
       "      <td>I Voice</td>\n",
       "      <td>“I awoke to two sweaty, meaty hands shaking th...</td>\n",
       "      <td>Q1150792</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curiosity</td>\n",
       "      <td>Suspense</td>\n",
       "      <td>Alaska finished her cigarette and flicked it i...</td>\n",
       "      <td>Q1150792</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>914</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Confusion</td>\n",
       "      <td>xxx-I need to enter something new</td>\n",
       "      <td>When she awakes one morning, Ariadne inquires ...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>913</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Plot Twist</td>\n",
       "      <td>Cassie swings the sword at Helen's neck and al...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Identification</td>\n",
       "      <td>Stretch</td>\n",
       "      <td>The emotions of a fictional, fantastical chara...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  author       date benefit      experience  \\\n",
       "0  916      10 2025-06-05     NaN       Confusion   \n",
       "1  915      10 2025-06-05     NaN       Curiosity   \n",
       "2  914      10 2025-06-05     NaN       Confusion   \n",
       "3  913      10 2025-06-05     NaN          Wonder   \n",
       "4  912      10 2025-06-05     NaN  Identification   \n",
       "\n",
       "                          technology  \\\n",
       "0                            I Voice   \n",
       "1                           Suspense   \n",
       "2  xxx-I need to enter something new   \n",
       "3                         Plot Twist   \n",
       "4                            Stretch   \n",
       "\n",
       "                                                text       QID  \\\n",
       "0  “I awoke to two sweaty, meaty hands shaking th...  Q1150792   \n",
       "1  Alaska finished her cigarette and flicked it i...  Q1150792   \n",
       "2  When she awakes one morning, Ariadne inquires ...  Q7601547   \n",
       "3  Cassie swings the sword at Helen's neck and al...  Q7601547   \n",
       "4  The emotions of a fictional, fantastical chara...  Q7601547   \n",
       "\n",
       "                title  \n",
       "0  Looking for Alaska  \n",
       "1  Looking for Alaska  \n",
       "2         Starcrossed  \n",
       "3         Starcrossed  \n",
       "4         Starcrossed  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "WonderCat\n",
    "\"\"\"\n",
    "# Call Data from WordPress API\n",
    "wp_call = read_wordpress_post_with_pagination()\n",
    "\n",
    "# Reshape wp_call (json) as dataframe.\n",
    "data = transform_to_dataframe(wp_call)\n",
    "\n",
    "\"\"\"\n",
    "WikiData\n",
    "\"\"\"\n",
    "# Get QIDS.\n",
    "qids = get_QIDS(data)\n",
    "\n",
    "# Call Wikidata API.\n",
    "api_results = build_query_call_api(qids)\n",
    "\n",
    "# Convert API data to dataframe.\n",
    "wikidata = api_to_dataframe(api_results)\n",
    "\n",
    "# Merge data so WonderCat uses WikiData title.\n",
    "data = pd.merge(data, wikidata[['itemLabel', 'QID']], on = \"QID\")\n",
    "data.rename(columns={'itemLabel':'title'}, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Returned Data\n",
    "\"\"\"\n",
    "# wikidata.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 6.92 ms, total: 130 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    tech_exp = dataframe[['technology', 'experience']]\n",
    "    tech_exp.rename(columns = {'technology': 'from', 'experience': 'to'}, inplace = True)\n",
    "\n",
    "    exp_user = dataframe[['experience', 'author']]\n",
    "    exp_user.rename(columns = {'experience': 'from', 'author': 'to'}, inplace = True)\n",
    "\n",
    "    # Join pairs.\n",
    "    links = pd.concat([title_tech, tech_exp, exp_user]) \n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace(r'\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace(r'\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = r\"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = r\"\\t\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

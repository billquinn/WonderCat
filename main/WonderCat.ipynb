{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WonderCat Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, warnings, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 μs, sys: 1e+03 ns, total: 9 μs\n",
      "Wall time: 16 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "WordPress API Credentials and Functions\n",
    "\"\"\"\n",
    "api_prefix = 'https://env-1120817.us.reclaim.cloud/wp-json/wp/v2/user-experience'\n",
    "\n",
    "def get_total_pagecount():\n",
    "    api_url = f'{api_prefix}?page=1&per_page=100'\n",
    "    response = requests.get(api_url)\n",
    "    pages_count = response.headers['X-WP-TotalPages']\n",
    "    return int(pages_count)\n",
    "\n",
    "def read_wordpress_post_with_pagination():\n",
    "    total_pages = get_total_pagecount()\n",
    "    current_page = 1\n",
    "    all_page_items_json = []\n",
    "    while current_page <= total_pages:\n",
    "        api_url = f\"{api_prefix}?page={current_page}&per_page=100\"\n",
    "        page_items = requests.get(api_url)\n",
    "        page_items_json = page_items.json()\n",
    "        all_page_items_json.extend(page_items_json)\n",
    "        current_page = current_page + 1\n",
    "    return all_page_items_json\n",
    "\n",
    "\"\"\"\n",
    "Transform API JSON to Dataframe\n",
    "\"\"\"\n",
    "def transform_to_dataframe(api_call):\n",
    "    api_data = pd.DataFrame(api_call)\n",
    "    api_data = api_data[['id', 'author', 'date', 'benefit', 'experience', 'technology', 'acf']] # Select columns to work with. Add 'wikidata' when ready.\n",
    "    api_data['title'] = pd.json_normalize(api_data['acf'])['title_of_creative_work']\n",
    "    api_data['QID'] = pd.json_normalize(api_data['acf'])['wikidata-qid']\n",
    "    # This should be cleaner...\n",
    "    api_data['bene_del'] = pd.json_normalize(api_data['benefit'])\n",
    "    api_data['benefit'] = pd.json_normalize(api_data['bene_del'])['name']\n",
    "    api_data['exp_del'] = pd.json_normalize(api_data['experience'])\n",
    "    api_data['experience'] = pd.json_normalize(api_data['exp_del'])['name']\n",
    "    api_data['tech_del'] = pd.json_normalize(api_data['technology'])\n",
    "    api_data['technology'] = pd.json_normalize(api_data['tech_del'])['name']\n",
    "    del api_data['acf'], api_data['bene_del'], api_data['exp_del'], api_data['tech_del']\n",
    "\n",
    "    # Convert date of experience to Y-m-d\n",
    "    api_data['date'] = api_data['date'].str.replace('(\\d{4}-\\d{2}-\\d{2}).*', '\\\\1', regex = True)\n",
    "    api_data['date'] = pd.to_datetime(api_data['date'])\n",
    "\n",
    "\n",
    "    return api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write WonderCat API Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.51 s, sys: 693 ms, total: 8.2 s\n",
      "Wall time: 14.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>benefit</th>\n",
       "      <th>experience</th>\n",
       "      <th>technology</th>\n",
       "      <th>title</th>\n",
       "      <th>QID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>362</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>Faith</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Enigma</td>\n",
       "      <td>Mystery Plays</td>\n",
       "      <td>Q240911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>Generosity</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Stretch</td>\n",
       "      <td>Oedipus</td>\n",
       "      <td>Q148643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>Faith</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Plot Twist</td>\n",
       "      <td>Oedipus</td>\n",
       "      <td>Q148643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>361</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>Peace of Mind</td>\n",
       "      <td>Tranquility</td>\n",
       "      <td>Stream of Consciousness</td>\n",
       "      <td>Me Before You</td>\n",
       "      <td>Q20657314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-01-09</td>\n",
       "      <td>Peace of Mind</td>\n",
       "      <td>Tranquility</td>\n",
       "      <td>Stream of Consciousness</td>\n",
       "      <td>The Crying of Lot 49</td>\n",
       "      <td>Q2344707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  author       date        benefit   experience  \\\n",
       "0  362       5 2025-01-09          Faith       Wonder   \n",
       "1  364       5 2025-01-09     Generosity       Wonder   \n",
       "2  363       5 2025-01-09          Faith       Wonder   \n",
       "3  361       5 2025-01-09  Peace of Mind  Tranquility   \n",
       "4  360       5 2025-01-09  Peace of Mind  Tranquility   \n",
       "\n",
       "                technology                 title        QID  \n",
       "0                   Enigma         Mystery Plays    Q240911  \n",
       "1                  Stretch               Oedipus    Q148643  \n",
       "2               Plot Twist               Oedipus    Q148643  \n",
       "3  Stream of Consciousness         Me Before You  Q20657314  \n",
       "4  Stream of Consciousness  The Crying of Lot 49   Q2344707  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Call Data from WordPress API\n",
    "wp_call = read_wordpress_post_with_pagination()\n",
    "\n",
    "# Reshape wp_call (json) as dataframe.\n",
    "data = transform_to_dataframe(wp_call)\n",
    "\n",
    "# Write to file.\n",
    "data.to_csv(\"wonderCat_data.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiData Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 μs, sys: 1 μs, total: 13 μs\n",
      "Wall time: 15 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather all QID's from dataframe.\n",
    "def get_QIDS(df):\n",
    "    # Gather QIDS and validate with regular expression.\n",
    "    QIDS = df['QID'].unique()\n",
    "    regex = re.compile('Q\\d+')\n",
    "    QIDS = [s for s in QIDS if regex.match(s)]\n",
    "\n",
    "    # Append 'wd:' prefix for sparql query.\n",
    "    QIDS = ' '.join(['wd:' + x for x in QIDS if isinstance(x, str)])\n",
    "\n",
    "    return QIDS\n",
    "\n",
    "\n",
    "# Build SPARQL query.\n",
    "def build_query_call_api(QIDS):\n",
    "    QIDS = QIDS\n",
    "\n",
    "    # Build SPARQL Query.\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        ?item ?pubDate ?genreLabel\n",
    "        ?countryOriginLabel ?coordinates\n",
    "\n",
    "    WHERE {\n",
    "        VALUES ?item { %s }\n",
    "\n",
    "        ?item wdt:P31 ?instanceof.\n",
    "        OPTIONAL {?item wdt:P136 ?genre}.\n",
    "        OPTIONAL {?item wdt:P577 ?pubDate}.\n",
    "        ?item wdt:P495 ?countryOrigin .\n",
    "        ?countryOrigin wdt:P625 ?coordinates.\n",
    "    \n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en,en\". }\n",
    "    }\n",
    "    \"\"\" % (QIDS)\n",
    "\n",
    "    # Call API\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    res = requests.get(url, params={'query': sparql_query, 'format': 'json'}).json()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create dataframe from API results.\n",
    "def api_to_dataframe(res):\n",
    "    wiki_df =[]\n",
    "\n",
    "    # Loop through WikiQuery Results.\n",
    "    for i in res['results']['bindings']:\n",
    "        # Build empty dictionary.\n",
    "        wiki_item = {}\n",
    "        # Loop through each item's keys.\n",
    "        for k in i.keys():\n",
    "            # Append values to wiki_item\n",
    "            wiki_item[k] = i[k]['value']\n",
    "\n",
    "        # Once item's keys looped, append new dictionary to list for dataframe.\n",
    "        wiki_df.append(wiki_item)\n",
    "\n",
    "    wiki_df = pd.DataFrame(wiki_df)\n",
    "\n",
    "    # Clean up item/QID field.\n",
    "    wiki_df['item'] = wiki_df['item'].str.replace('.*/(Q\\d+)', '\\\\1', regex = True)\n",
    "    wiki_df = wiki_df.rename(columns = {'item':'QID'})\n",
    "\n",
    "    # Clean up date field. Currently returning only year due to some dates being \"out of bounds\" (too old).\n",
    "    wiki_df['pubDate'] = wiki_df['pubDate'].str.replace('(\\d{4}-\\d{2}-\\d{2}).*', '\\\\1', regex = True)\n",
    "    wiki_df['pubDate'] = pd.to_datetime(wiki_df['pubDate'], errors = 'coerce')\n",
    "\n",
    "    # Create Longitude and Latitude columns.\n",
    "    reg_pattern = 'Point\\(([-]?\\d+\\.?\\d+)\\s([-]?\\d+\\.?\\d+)\\)'\n",
    "    wiki_df['long'] = wiki_df['coordinates'].str.replace(reg_pattern, '\\\\1', regex = True)\n",
    "    wiki_df['lat'] = wiki_df['coordinates'].str.replace(reg_pattern, '\\\\2', regex = True)\n",
    "\n",
    "    # # Convert rows of genres into single value (list)\n",
    "    # wiki_df = wiki_df.groupby(['QID', 'long', 'lat'], as_index=False) \\\n",
    "    #     .agg({'genreLabel': lambda x: x.tolist(), 'pubDate': lambda x: x.tolist()})\n",
    "\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID                   False\n",
      "title                 False\n",
      "coordinates           False\n",
      "countryOriginLabel    False\n",
      "genreLabel            False\n",
      "pubDate               False\n",
      "long                  False\n",
      "lat                   False\n",
      "dtype: bool\n",
      "CPU times: user 56.2 ms, sys: 6.84 ms, total: 63 ms\n",
      "Wall time: 439 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>title</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>countryOriginLabel</th>\n",
       "      <th>genreLabel</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q20657314</td>\n",
       "      <td>Me Before You</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>United States</td>\n",
       "      <td>romantic fiction</td>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2344707</td>\n",
       "      <td>The Crying of Lot 49</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>United States</td>\n",
       "      <td>science fiction</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q2344707</td>\n",
       "      <td>The Crying of Lot 49</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>United States</td>\n",
       "      <td>secret history</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q2344707</td>\n",
       "      <td>The Crying of Lot 49</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>United States</td>\n",
       "      <td>metafiction</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2344707</td>\n",
       "      <td>The Crying of Lot 49</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>United States</td>\n",
       "      <td>paranoid fiction</td>\n",
       "      <td>1966-01-01</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QID                 title                coordinates  \\\n",
       "0  Q20657314         Me Before You  Point(-98.5795 39.828175)   \n",
       "1   Q2344707  The Crying of Lot 49  Point(-98.5795 39.828175)   \n",
       "2   Q2344707  The Crying of Lot 49  Point(-98.5795 39.828175)   \n",
       "3   Q2344707  The Crying of Lot 49  Point(-98.5795 39.828175)   \n",
       "4   Q2344707  The Crying of Lot 49  Point(-98.5795 39.828175)   \n",
       "\n",
       "  countryOriginLabel        genreLabel    pubDate      long        lat  \n",
       "0      United States  romantic fiction 2016-06-24  -98.5795  39.828175  \n",
       "1      United States   science fiction 1966-01-01  -98.5795  39.828175  \n",
       "2      United States    secret history 1966-01-01  -98.5795  39.828175  \n",
       "3      United States       metafiction 1966-01-01  -98.5795  39.828175  \n",
       "4      United States  paranoid fiction 1966-01-01  -98.5795  39.828175  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get QIDS.\n",
    "qids = get_QIDS(data)\n",
    "\n",
    "# Call Wikidata API.\n",
    "api_results = build_query_call_api(qids)\n",
    "\n",
    "# Convert API data to dataframe.\n",
    "wikidata = api_to_dataframe(api_results)\n",
    "\n",
    "# Merge with WonderCat dataframe.\n",
    "wikidata = data[['QID', 'title']].merge(wikidata, how = 'inner', on = 'QID')\n",
    "\n",
    "# Save dataframe as .tsv\n",
    "wikidata.to_csv(\"wikidata.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# See if columns that have lists are recognized.\n",
    "print (wikidata.map(lambda x: isinstance(x, list)).all())\n",
    "\n",
    "wikidata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Data with Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe, column1, column2):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace('\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace('\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = \"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 6.92 ms, total: 130 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    tech_exp = dataframe[['technology', 'experience']]\n",
    "    tech_exp.rename(columns = {'technology': 'from', 'experience': 'to'}, inplace = True)\n",
    "\n",
    "    exp_user = dataframe[['experience', 'author']]\n",
    "    exp_user.rename(columns = {'experience': 'from', 'author': 'to'}, inplace = True)\n",
    "\n",
    "    # Join pairs.\n",
    "    links = pd.concat([title_tech, tech_exp, exp_user]) \n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace('\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace('\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = \"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = \"\\t\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WonderCat Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, warnings, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 17.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather all QID's from dataframe.\n",
    "def get_QIDS(df):\n",
    "    # Gather QIDS and validate with regular expression.\n",
    "    QIDS = df['QID'].unique()\n",
    "    regex = re.compile(r'Q\\d+')\n",
    "    QIDS = [s for s in QIDS if regex.match(s)]\n",
    "\n",
    "    # Append 'wd:' prefix for sparql query.\n",
    "    QIDS = ' '.join(['wd:' + x for x in QIDS if isinstance(x, str)])\n",
    "\n",
    "    return QIDS\n",
    "\n",
    "\n",
    "# Build SPARQL query.\n",
    "def build_query_call_api(QIDS):\n",
    "    QIDS = QIDS\n",
    "\n",
    "    # Build SPARQL Query.\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        ?item ?itemLabel\n",
    "        (group_concat(DISTINCT(?dateLabel); separator=',') as ?pubDates)\n",
    "        (group_concat(DISTINCT(?genreLabel); separator=',') as ?genres)\n",
    "        (group_concat(DISTINCT(?countryOriginLabel); separator=',') as ?origin)\n",
    "        (group_concat(DISTINCT(?coordinatesLabel); separator=',') as ?coordinates)\n",
    "\n",
    "        WHERE {\n",
    "            VALUES ?item { %s }\n",
    "            ?item wdt:P31 ?instanceof.\n",
    "            OPTIONAL{?item wdt:P577 ?pubDate}.\n",
    "            OPTIONAL{?item wdt:P136 ?genre}.\n",
    "            ?item wdt:P495 ?origin.\n",
    "            ?origin wdt:P625 ?coordinates.\n",
    "\n",
    "            SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language 'en,en'.\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            ?pubDate rdfs:label ?dateLabel.\n",
    "            ?genre rdfs:label ?genreLabel.\n",
    "            ?origin rdfs:label ?countryOriginLabel.\n",
    "            ?coordinates rdfs:label ?coordinatesLabel.\n",
    "            }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (QIDS)\n",
    "\n",
    "    # Call API\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    res = requests.get(url, params={'query': sparql_query, 'format': 'json'}).json()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create dataframe from API results.\n",
    "def api_to_dataframe(res):\n",
    "    wiki_df =[]\n",
    "\n",
    "    # Loop through WikiQuery Results.\n",
    "    for i in res['results']['bindings']:\n",
    "        # Build empty dictionary.\n",
    "        wiki_item = {}\n",
    "        # Loop through each item's keys.\n",
    "        for k in i.keys():\n",
    "            # Append values to wiki_item\n",
    "            wiki_item[k] = i[k]['value']\n",
    "\n",
    "        # Once item's keys looped, append new dictionary to list for dataframe.\n",
    "        wiki_df.append(wiki_item)\n",
    "\n",
    "    wiki_df = pd.DataFrame(wiki_df)\n",
    "\n",
    "    # Clean up item/QID field.\n",
    "    wiki_df['item'] = wiki_df['item'].str.replace(r'.*/(Q\\d+)', '\\\\1', regex = True)\n",
    "    wiki_df = wiki_df.rename(columns = {'item':'QID'})\n",
    "\n",
    "    # Clean up date field. Currently returning only year due to some dates being \"out of bounds\" (too old).\n",
    "    wiki_df['pubDates'] = wiki_df['pubDates'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', r'\\\\1', regex = True)\n",
    "    wiki_df['pubDates'] = pd.to_datetime(wiki_df['pubDates'], errors = 'coerce')\n",
    "\n",
    "    # Create Longitude and Latitude columns.\n",
    "    reg_pattern = r'Point\\(([-]?\\d+\\.?\\d+)\\s([-]?\\d+\\.?\\d+)\\)'\n",
    "    wiki_df['lon'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\1', regex = True)\n",
    "    wiki_df['lat'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\2', regex = True)\n",
    "\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write WonderCat API Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 350 ms, total: 2.58 s\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>benefit</th>\n",
       "      <th>experience</th>\n",
       "      <th>technology</th>\n",
       "      <th>text</th>\n",
       "      <th>QID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>924</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-16</td>\n",
       "      <td>Belief</td>\n",
       "      <td>Identification</td>\n",
       "      <td>xxx-I need to enter something new</td>\n",
       "      <td>he loves her?! She loves him?! She has real fe...</td>\n",
       "      <td>Q125881442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>922</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>Being Wrong</td>\n",
       "      <td>Plot Twist</td>\n",
       "      <td>A completely unexpected twist immediately afte...</td>\n",
       "      <td>Q125881442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>921</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-06-15</td>\n",
       "      <td>Intimacy</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Secret Discloser</td>\n",
       "      <td>\"Have you ever had a really good day?\" (Escola...</td>\n",
       "      <td>Q125881442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>916</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Confusion</td>\n",
       "      <td>I Voice</td>\n",
       "      <td>“I awoke to two sweaty, meaty hands shaking th...</td>\n",
       "      <td>Q1150792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>915</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curiosity</td>\n",
       "      <td>Suspense</td>\n",
       "      <td>Alaska finished her cigarette and flicked it i...</td>\n",
       "      <td>Q1150792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  author       date     benefit      experience  \\\n",
       "0  924       1 2025-06-16      Belief  Identification   \n",
       "1  922       6 2025-06-15  Resilience     Being Wrong   \n",
       "2  921       6 2025-06-15    Intimacy          Wonder   \n",
       "3  916      10 2025-06-05         NaN       Confusion   \n",
       "4  915      10 2025-06-05         NaN       Curiosity   \n",
       "\n",
       "                          technology  \\\n",
       "0  xxx-I need to enter something new   \n",
       "1                         Plot Twist   \n",
       "2                   Secret Discloser   \n",
       "3                            I Voice   \n",
       "4                           Suspense   \n",
       "\n",
       "                                                text         QID  \n",
       "0  he loves her?! She loves him?! She has real fe...  Q125881442  \n",
       "1  A completely unexpected twist immediately afte...  Q125881442  \n",
       "2  \"Have you ever had a really good day?\" (Escola...  Q125881442  \n",
       "3  “I awoke to two sweaty, meaty hands shaking th...    Q1150792  \n",
       "4  Alaska finished her cigarette and flicked it i...    Q1150792  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Call Data from WordPress API\n",
    "wp_call = read_wordpress_post_with_pagination()\n",
    "\n",
    "# Reshape wp_call (json) as dataframe.\n",
    "data = transform_to_dataframe(wp_call)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiData Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather all QID's from dataframe.\n",
    "def get_QIDS(df):\n",
    "    # Gather QIDS and validate with regular expression.\n",
    "    QIDS = df['QID'].unique()\n",
    "    regex = re.compile('Q\\d+')\n",
    "    QIDS = [s for s in QIDS if regex.match(s)]\n",
    "\n",
    "    # Append 'wd:' prefix for sparql query.\n",
    "    QIDS = ' '.join(['wd:' + x for x in QIDS if isinstance(x, str)])\n",
    "\n",
    "    return QIDS\n",
    "\n",
    "\n",
    "# Build SPARQL query.\n",
    "def build_query_call_api(QIDS):\n",
    "    QIDS = QIDS\n",
    "\n",
    "    # Build SPARQL Query.\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        ?item ?itemLabel\n",
    "        (group_concat(DISTINCT(?dateLabel); separator=',') as ?pubDates)\n",
    "        (group_concat(DISTINCT(?genreLabel); separator=',') as ?genres)\n",
    "        (group_concat(DISTINCT(?countryOriginLabel); separator=',') as ?origin)\n",
    "        (group_concat(DISTINCT(?coordinatesLabel); separator=',') as ?coordinates)\n",
    "\n",
    "        WHERE {\n",
    "            VALUES ?item { %s }\n",
    "            ?item wdt:P31 ?instanceof.\n",
    "            OPTIONAL{?item wdt:P577 ?pubDate}.\n",
    "            OPTIONAL{?item wdt:P136 ?genre}.\n",
    "            ?item wdt:P495 ?origin.\n",
    "            ?origin wdt:P625 ?coordinates.\n",
    "\n",
    "            SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language 'en,en'.\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            ?pubDate rdfs:label ?dateLabel.\n",
    "            ?genre rdfs:label ?genreLabel.\n",
    "            ?origin rdfs:label ?countryOriginLabel.\n",
    "            ?coordinates rdfs:label ?coordinatesLabel.\n",
    "            }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (QIDS)\n",
    "\n",
    "    # Call API\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    res = requests.get(url, params={'query': sparql_query, 'format': 'json'}).json()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create dataframe from API results.\n",
    "def api_to_dataframe(res):\n",
    "    wiki_df =[]\n",
    "\n",
    "    # Loop through WikiQuery Results.\n",
    "    for i in res['results']['bindings']:\n",
    "        # Build empty dictionary.\n",
    "        wiki_item = {}\n",
    "        # Loop through each item's keys.\n",
    "        for k in i.keys():\n",
    "            # Append values to wiki_item\n",
    "            wiki_item[k] = i[k]['value']\n",
    "\n",
    "        # Once item's keys looped, append new dictionary to list for dataframe.\n",
    "        wiki_df.append(wiki_item)\n",
    "\n",
    "    wiki_df = pd.DataFrame(wiki_df)\n",
    "\n",
    "    # Clean up item/QID field.\n",
    "    wiki_df['item'] = wiki_df['item'].str.replace(r'.*/(Q\\d+)', r'\\1', regex = True)\n",
    "    wiki_df = wiki_df.rename(columns = {'item':'QID'})\n",
    "\n",
    "    # Clean up date field. Currently returning only year due to some dates being \"out of bounds\" (too old).\n",
    "    wiki_df['pubDates'] = wiki_df['pubDates'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', r'\\1', regex = True)\n",
    "    wiki_df['pubDates'] = pd.to_datetime(wiki_df['pubDates'], errors = 'coerce')\n",
    "\n",
    "    # Create Longitude and Latitude columns.\n",
    "    reg_pattern = r'Point\\(([-]?\\d+\\.?\\d+)\\s([-]?\\d+\\.?\\d+)\\)'\n",
    "    wiki_df['lon'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\1', regex = True)\n",
    "    wiki_df['lat'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\2', regex = True)\n",
    "\n",
    "    return wiki_df\n",
    "\n",
    "    # # (Old method for concatenating genres) Concatenate genres.\n",
    "    # genres = wikidata[['QID', 'genreLabel']]\n",
    "    # genres['genreLabel'].replace('', np.nan, inplace = True)\n",
    "    # genres.dropna(subset=['genreLabel'], inplace=True)\n",
    "    # genres = genres.groupby('QID')['genreLabel'].apply(lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "    # # Re-merge concatenated rows with rest of data.\n",
    "    # wiki_df = wikidata.drop(['genreLabel'], axis=1).merge(genres, how = 'inner', on = 'QID')\n",
    "\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 ms, sys: 2.83 ms, total: 14.5 ms\n",
      "Wall time: 74.9 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>itemLabel</th>\n",
       "      <th>pubDates</th>\n",
       "      <th>genres</th>\n",
       "      <th>origin</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1170769</td>\n",
       "      <td>The Essence of Christianity</td>\n",
       "      <td>1841-01-01</td>\n",
       "      <td></td>\n",
       "      <td>Germany</td>\n",
       "      <td>Point(10.0 51.0)</td>\n",
       "      <td>10.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2446285</td>\n",
       "      <td>Pippi Longstocking</td>\n",
       "      <td>1945-11-01</td>\n",
       "      <td>fantasy,children's fiction</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>Point(15.0 61.0)</td>\n",
       "      <td>15.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q16733998</td>\n",
       "      <td>Our Sister Killjoy</td>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>fiction</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>Point(-1.08 8.03),Point(-1.2 8.1)</td>\n",
       "      <td>-1.08,-1.2</td>\n",
       "      <td>8.03,8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q5477055</td>\n",
       "      <td>Fox in Socks</td>\n",
       "      <td>1965-06-19</td>\n",
       "      <td></td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q265954</td>\n",
       "      <td>Slaughterhouse-Five</td>\n",
       "      <td>1969-03-01</td>\n",
       "      <td>science fiction,black comedy,metafiction,anti-...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>-98.5795</td>\n",
       "      <td>39.828175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         QID                    itemLabel   pubDates  \\\n",
       "0   Q1170769  The Essence of Christianity 1841-01-01   \n",
       "1   Q2446285           Pippi Longstocking 1945-11-01   \n",
       "2  Q16733998           Our Sister Killjoy 1977-01-01   \n",
       "3   Q5477055                 Fox in Socks 1965-06-19   \n",
       "4    Q265954          Slaughterhouse-Five 1969-03-01   \n",
       "\n",
       "                                              genres         origin  \\\n",
       "0                                                           Germany   \n",
       "1                         fantasy,children's fiction         Sweden   \n",
       "2                                            fiction          Ghana   \n",
       "3                                                     United States   \n",
       "4  science fiction,black comedy,metafiction,anti-...  United States   \n",
       "\n",
       "                         coordinates         lon        lat  \n",
       "0                   Point(10.0 51.0)        10.0       51.0  \n",
       "1                   Point(15.0 61.0)        15.0       61.0  \n",
       "2  Point(-1.08 8.03),Point(-1.2 8.1)  -1.08,-1.2   8.03,8.1  \n",
       "3          Point(-98.5795 39.828175)    -98.5795  39.828175  \n",
       "4          Point(-98.5795 39.828175)    -98.5795  39.828175  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get QIDS.\n",
    "qids = get_QIDS(data)\n",
    "\n",
    "# Call Wikidata API.\n",
    "api_results = build_query_call_api(qids)\n",
    "\n",
    "# Convert API data to dataframe.\n",
    "wikidata = api_to_dataframe(api_results)\n",
    "\n",
    "# # Merge with WonderCat dataframe.\n",
    "# wikidata = data[['QID', 'title']].merge(wikidata, how = 'inner', on = 'QID')\n",
    "\n",
    "# # Save dataframe as .tsv\n",
    "# wikidata.to_csv(\"wikidata.tsv\", sep = \"\\t\", index = False)\n",
    "\n",
    "# # See if columns that have lists are recognized.\n",
    "# print (wikidata.map(lambda x: isinstance(x, list)).all())\n",
    "\n",
    "wikidata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Data with Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe, column1, column2):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace('\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace('\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = \"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 6.92 ms, total: 130 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    tech_exp = dataframe[['technology', 'experience']]\n",
    "    tech_exp.rename(columns = {'technology': 'from', 'experience': 'to'}, inplace = True)\n",
    "\n",
    "    exp_user = dataframe[['experience', 'author']]\n",
    "    exp_user.rename(columns = {'experience': 'from', 'author': 'to'}, inplace = True)\n",
    "\n",
    "    # Join pairs.\n",
    "    links = pd.concat([title_tech, tech_exp, exp_user]) \n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace('\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace('\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = \"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = \"\\t\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

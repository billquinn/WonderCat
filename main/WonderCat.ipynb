{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WonderCat Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, base64, warnings, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call API and Store Data\n",
    "\n",
    "### WonderCat API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Fetch WonderCat Data through API\n",
    "api_prefix = 'https://env-1120817.us.reclaim.cloud/wp-json/wp/v2/user-experience'\n",
    "\n",
    "def get_total_pagecount():\n",
    "    api_url = f'{api_prefix}?page=1&per_page=100'\n",
    "    response = requests.get(api_url)\n",
    "    pages_count = response.headers['X-WP-TotalPages']\n",
    "    return int(pages_count)\n",
    "\n",
    "def read_wordpress_post_with_pagination():\n",
    "    total_pages = get_total_pagecount()\n",
    "    current_page = 1\n",
    "    all_page_items_json = []\n",
    "    while current_page <= total_pages:\n",
    "        api_url = f\"{api_prefix}?page={current_page}&per_page=100\"\n",
    "        page_items = requests.get(api_url)\n",
    "        page_items_json = page_items.json()\n",
    "        all_page_items_json.extend(page_items_json)\n",
    "        current_page = current_page + 1\n",
    "    return all_page_items_json\n",
    "\n",
    "# Transform API JSON to Dataframe\n",
    "def transform_to_dataframe(api_call):\n",
    "    api_data = pd.DataFrame(api_call)\n",
    "    api_data = api_data[['id', 'author', 'date', 'benefit', 'experience', 'technology', 'acf']]\n",
    "    # This should be cleaner...\n",
    "    api_data['bene_del'] = pd.json_normalize(api_data['benefit'])\n",
    "    api_data['benefit'] = pd.json_normalize(api_data['bene_del'])['name']\n",
    "    api_data['exp_del'] = pd.json_normalize(api_data['experience'])\n",
    "    api_data['experience'] = pd.json_normalize(api_data['exp_del'])['name']\n",
    "    api_data['tech_del'] = pd.json_normalize(api_data['technology'])\n",
    "    api_data['technology'] = pd.json_normalize(api_data['tech_del'])['name']\n",
    "    api_data['text'] = pd.json_normalize(api_data['acf'])['feature']\n",
    "    api_data['QID'] = pd.json_normalize(api_data['acf'])['wikidata-qid']\n",
    "    del api_data['acf'], api_data['bene_del'], api_data['exp_del'], api_data['tech_del']\n",
    "\n",
    "    # Convert date of experience to Y-m-d\n",
    "    api_data['date'] = api_data['date'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', '\\\\1', regex = True)\n",
    "    api_data['date'] = pd.to_datetime(api_data['date'])\n",
    "\n",
    "    return api_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiData API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather all QID's from dataframe.\n",
    "def get_QIDS(df):\n",
    "    # Gather QIDS and validate with regular expression.\n",
    "    QIDS = df['QID'].unique()\n",
    "    regex = re.compile(r'Q\\d+')\n",
    "    QIDS = [s for s in QIDS if regex.match(s)]\n",
    "\n",
    "    # Append 'wd:' prefix for sparql query.\n",
    "    QIDS = ' '.join(['wd:' + x for x in QIDS if isinstance(x, str)])\n",
    "\n",
    "    return QIDS\n",
    "\n",
    "\n",
    "# Build SPARQL query.\n",
    "def build_query_call_api(QIDS):\n",
    "    QIDS = QIDS\n",
    "\n",
    "    # Build SPARQL Query.\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        ?item ?itemLabel\n",
    "        (group_concat(DISTINCT(?dateLabel); separator=',') as ?pubDates)\n",
    "        (group_concat(DISTINCT(?genreLabel); separator=',') as ?genres)\n",
    "        (group_concat(DISTINCT(?countryOriginLabel); separator=',') as ?origin)\n",
    "        (group_concat(DISTINCT(?coordinatesLabel); separator=',') as ?coordinates)\n",
    "\n",
    "        WHERE {\n",
    "            VALUES ?item { %s }\n",
    "            ?item wdt:P31 ?instanceof.\n",
    "            OPTIONAL{?item wdt:P577 ?pubDate}.\n",
    "            OPTIONAL{?item wdt:P136 ?genre}.\n",
    "            ?item wdt:P495 ?origin.\n",
    "            ?origin wdt:P625 ?coordinates.\n",
    "\n",
    "            SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language 'en,en'.\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            ?pubDate rdfs:label ?dateLabel.\n",
    "            ?genre rdfs:label ?genreLabel.\n",
    "            ?origin rdfs:label ?countryOriginLabel.\n",
    "            ?coordinates rdfs:label ?coordinatesLabel.\n",
    "            }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (QIDS)\n",
    "\n",
    "    # Call API\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    res = requests.get(url, params={'query': sparql_query, 'format': 'json'}).json()\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create dataframe from API results.\n",
    "def api_to_dataframe(res):\n",
    "    wiki_df =[]\n",
    "\n",
    "    # Loop through WikiQuery Results.\n",
    "    for i in res['results']['bindings']:\n",
    "        # Build empty dictionary.\n",
    "        wiki_item = {}\n",
    "        # Loop through each item's keys.\n",
    "        for k in i.keys():\n",
    "            # Append values to wiki_item\n",
    "            wiki_item[k] = i[k]['value']\n",
    "\n",
    "        # Once item's keys looped, append new dictionary to list for dataframe.\n",
    "        wiki_df.append(wiki_item)\n",
    "\n",
    "    wiki_df = pd.DataFrame(wiki_df)\n",
    "\n",
    "    # Clean up item/QID field.\n",
    "    wiki_df['item'] = wiki_df['item'].str.replace(r'.*/(Q\\d+)', '\\\\1', regex = True)\n",
    "    wiki_df = wiki_df.rename(columns = {'item':'QID'})\n",
    "\n",
    "    # Clean up date field. Currently returning only year due to some dates being \"out of bounds\" (too old).\n",
    "    wiki_df['pubDates'] = wiki_df['pubDates'].str.replace(r'(\\d{4}-\\d{2}-\\d{2}).*', r'\\\\1', regex = True)\n",
    "    wiki_df['pubDates'] = pd.to_datetime(wiki_df['pubDates'], errors = 'coerce')\n",
    "\n",
    "    # Create Longitude and Latitude columns.\n",
    "    reg_pattern = r'Point\\(([-]?\\d+\\.?\\d+)\\s([-]?\\d+\\.?\\d+)\\)'\n",
    "    wiki_df['lon'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\1', regex = True)\n",
    "    wiki_df['lat'] = wiki_df['coordinates'].str.replace(reg_pattern, r'\\\\2', regex = True)\n",
    "\n",
    "    return wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write WonderCat API Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 411 ms, total: 2.69 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>benefit</th>\n",
       "      <th>experience</th>\n",
       "      <th>technology</th>\n",
       "      <th>text</th>\n",
       "      <th>QID</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>916</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Confusion</td>\n",
       "      <td>I Voice</td>\n",
       "      <td>“I awoke to two sweaty, meaty hands shaking th...</td>\n",
       "      <td>Q1150792</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curiosity</td>\n",
       "      <td>Suspense</td>\n",
       "      <td>Alaska finished her cigarette and flicked it i...</td>\n",
       "      <td>Q1150792</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>914</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Confusion</td>\n",
       "      <td>xxx-I need to enter something new</td>\n",
       "      <td>When she awakes one morning, Ariadne inquires ...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>913</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wonder</td>\n",
       "      <td>Plot Twist</td>\n",
       "      <td>Cassie swings the sword at Helen's neck and al...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>912</td>\n",
       "      <td>10</td>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Identification</td>\n",
       "      <td>Stretch</td>\n",
       "      <td>The emotions of a fictional, fantastical chara...</td>\n",
       "      <td>Q7601547</td>\n",
       "      <td>Starcrossed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  author       date benefit      experience  \\\n",
       "0  916      10 2025-06-05     NaN       Confusion   \n",
       "1  915      10 2025-06-05     NaN       Curiosity   \n",
       "2  914      10 2025-06-05     NaN       Confusion   \n",
       "3  913      10 2025-06-05     NaN          Wonder   \n",
       "4  912      10 2025-06-05     NaN  Identification   \n",
       "\n",
       "                          technology  \\\n",
       "0                            I Voice   \n",
       "1                           Suspense   \n",
       "2  xxx-I need to enter something new   \n",
       "3                         Plot Twist   \n",
       "4                            Stretch   \n",
       "\n",
       "                                                text       QID  \\\n",
       "0  “I awoke to two sweaty, meaty hands shaking th...  Q1150792   \n",
       "1  Alaska finished her cigarette and flicked it i...  Q1150792   \n",
       "2  When she awakes one morning, Ariadne inquires ...  Q7601547   \n",
       "3  Cassie swings the sword at Helen's neck and al...  Q7601547   \n",
       "4  The emotions of a fictional, fantastical chara...  Q7601547   \n",
       "\n",
       "                title  \n",
       "0  Looking for Alaska  \n",
       "1  Looking for Alaska  \n",
       "2         Starcrossed  \n",
       "3         Starcrossed  \n",
       "4         Starcrossed  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "WonderCat\n",
    "\"\"\"\n",
    "# Call Data from WordPress API\n",
    "wp_call = read_wordpress_post_with_pagination()\n",
    "\n",
    "# Reshape wp_call (json) as dataframe.\n",
    "data = transform_to_dataframe(wp_call)\n",
    "\n",
    "\"\"\"\n",
    "WikiData\n",
    "\"\"\"\n",
    "# Get QIDS.\n",
    "qids = get_QIDS(data)\n",
    "\n",
    "# Call Wikidata API.\n",
    "api_results = build_query_call_api(qids)\n",
    "\n",
    "# Convert API data to dataframe.\n",
    "wikidata = api_to_dataframe(api_results)\n",
    "\n",
    "# Merge data so WonderCat uses WikiData title.\n",
    "data = pd.merge(data, wikidata[['itemLabel', 'QID']], on = \"QID\")\n",
    "data.rename(columns={'itemLabel':'title'}, inplace=True)\n",
    "\n",
    "\"\"\"\n",
    "Returned Data\n",
    "\"\"\"\n",
    "# wikidata.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>itemLabel</th>\n",
       "      <th>pubDates</th>\n",
       "      <th>genres</th>\n",
       "      <th>origin</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q2776448</td>\n",
       "      <td>My Ántonia</td>\n",
       "      <td>NaT</td>\n",
       "      <td>historical fiction</td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q135465</td>\n",
       "      <td>Rashomon</td>\n",
       "      <td>NaT</td>\n",
       "      <td>drama film,samurai cinema,crime film,medieval ...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Point(136.0 35.0)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1136104</td>\n",
       "      <td>Le Morte d'Arthur</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Arthurian romance,chivalric romance</td>\n",
       "      <td>England</td>\n",
       "      <td>Point(-1.0 53.0)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1215018</td>\n",
       "      <td>The Mill on the Floss</td>\n",
       "      <td>NaT</td>\n",
       "      <td>psychological fiction</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Point(-2.0 54.6)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6467743</td>\n",
       "      <td>Labyrinths</td>\n",
       "      <td>NaT</td>\n",
       "      <td>essay,magic realist fiction</td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Q3221351</td>\n",
       "      <td>The Calcutta Chromosome</td>\n",
       "      <td>NaT</td>\n",
       "      <td>science fiction,thriller</td>\n",
       "      <td>India</td>\n",
       "      <td>Point(83.0 22.8)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Q172230</td>\n",
       "      <td>Call the Midwife</td>\n",
       "      <td>NaT</td>\n",
       "      <td>drama television series,medical drama,historic...</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Point(-2.0 54.6)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Q50282028</td>\n",
       "      <td>Becoming</td>\n",
       "      <td>NaT</td>\n",
       "      <td>memoir</td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Q993664</td>\n",
       "      <td>The Year of Magical Thinking</td>\n",
       "      <td>NaT</td>\n",
       "      <td>memoir</td>\n",
       "      <td>United States</td>\n",
       "      <td>Point(-98.5795 39.828175)</td>\n",
       "      <td>\\1</td>\n",
       "      <td>\\2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Q125622088</td>\n",
       "      <td>Flow</td>\n",
       "      <td>NaT</td>\n",
       "      <td>fantasy film,adventure film,family film</td>\n",
       "      <td>Belgium,France,Latvia</td>\n",
       "      <td>Point(4.668055555 50.641111111),Point(2.0 47.0...</td>\n",
       "      <td>\\1,\\1,\\1</td>\n",
       "      <td>\\2,\\2,\\2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            QID                     itemLabel pubDates  \\\n",
       "0      Q2776448                    My Ántonia      NaT   \n",
       "1       Q135465                      Rashomon      NaT   \n",
       "2      Q1136104             Le Morte d'Arthur      NaT   \n",
       "3      Q1215018         The Mill on the Floss      NaT   \n",
       "4      Q6467743                    Labyrinths      NaT   \n",
       "..          ...                           ...      ...   \n",
       "174    Q3221351       The Calcutta Chromosome      NaT   \n",
       "175     Q172230              Call the Midwife      NaT   \n",
       "176   Q50282028                      Becoming      NaT   \n",
       "177     Q993664  The Year of Magical Thinking      NaT   \n",
       "178  Q125622088                          Flow      NaT   \n",
       "\n",
       "                                                genres                 origin  \\\n",
       "0                                   historical fiction          United States   \n",
       "1    drama film,samurai cinema,crime film,medieval ...                  Japan   \n",
       "2                  Arthurian romance,chivalric romance                England   \n",
       "3                                psychological fiction         United Kingdom   \n",
       "4                          essay,magic realist fiction          United States   \n",
       "..                                                 ...                    ...   \n",
       "174                           science fiction,thriller                  India   \n",
       "175  drama television series,medical drama,historic...         United Kingdom   \n",
       "176                                             memoir          United States   \n",
       "177                                             memoir          United States   \n",
       "178            fantasy film,adventure film,family film  Belgium,France,Latvia   \n",
       "\n",
       "                                           coordinates       lon       lat  \n",
       "0                            Point(-98.5795 39.828175)        \\1        \\2  \n",
       "1                                    Point(136.0 35.0)        \\1        \\2  \n",
       "2                                     Point(-1.0 53.0)        \\1        \\2  \n",
       "3                                     Point(-2.0 54.6)        \\1        \\2  \n",
       "4                            Point(-98.5795 39.828175)        \\1        \\2  \n",
       "..                                                 ...       ...       ...  \n",
       "174                                   Point(83.0 22.8)        \\1        \\2  \n",
       "175                                   Point(-2.0 54.6)        \\1        \\2  \n",
       "176                          Point(-98.5795 39.828175)        \\1        \\2  \n",
       "177                          Point(-98.5795 39.828175)        \\1        \\2  \n",
       "178  Point(4.668055555 50.641111111),Point(2.0 47.0...  \\1,\\1,\\1  \\2,\\2,\\2  \n",
       "\n",
       "[179 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Data with Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe, column1, column2):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace('\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace('\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = \"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data for Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 6.92 ms, total: 130 ms\n",
      "Wall time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_nodes_and_links(dataframe):\n",
    "    # Create link/edge pairs.\n",
    "    title_tech = dataframe[['title', 'technology']]\n",
    "    title_tech.rename(columns = {'title': 'from', 'technology': 'to'}, inplace = True)\n",
    "\n",
    "    tech_exp = dataframe[['technology', 'experience']]\n",
    "    tech_exp.rename(columns = {'technology': 'from', 'experience': 'to'}, inplace = True)\n",
    "\n",
    "    exp_user = dataframe[['experience', 'author']]\n",
    "    exp_user.rename(columns = {'experience': 'from', 'author': 'to'}, inplace = True)\n",
    "\n",
    "    # Join pairs.\n",
    "    links = pd.concat([title_tech, tech_exp, exp_user]) \n",
    "\n",
    "    # Clean pairs of whitespace.\n",
    "    links['from'] = links['from'].str.replace(r'\\\\w', '')\n",
    "    links['to'] = links['to'].str.replace(r'\\\\w', '')\n",
    "\n",
    "    # Create link/edge weights.\n",
    "    links = links.groupby(['from', 'to']).size().to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    # Create nodes from links and rename column name.\n",
    "    titles = dataframe[['title']]\n",
    "    titles.rename(columns = {'title': 'label'}, inplace = True)\n",
    "    titles['category'] = 'title'\n",
    "\n",
    "    technologies = dataframe[['technology']]\n",
    "    technologies.rename(columns = {'technology': 'label'}, inplace = True)\n",
    "    technologies['category'] = 'technology'\n",
    "\n",
    "    experiences = dataframe[['experience']]\n",
    "    experiences.rename(columns = {'experience': 'label'}, inplace = True)\n",
    "    experiences['category'] = 'experience'\n",
    "\n",
    "    users = dataframe[[\"author\"]]\n",
    "    users.rename(columns = {'author': 'label'}, inplace = True)\n",
    "    users['category'] = 'user'\n",
    "\n",
    "    # Concatenate nodes.\n",
    "    nodes = pd.concat([titles, technologies, experiences, users]) # users\n",
    "\n",
    "    # Create node \"size\" from frequency.\n",
    "    nodes = nodes.groupby(['label', 'category']).size().to_frame(name = 'size').reset_index()\n",
    "\n",
    "    # Remove duplicates from nodes.\n",
    "    nodes.drop_duplicates(inplace = True)\n",
    "\n",
    "    # Create node \"id's.\"\n",
    "    nodes['id'] = nodes.index\n",
    "\n",
    "    # Replace link's 'labels' with node id's.\n",
    "    label_id_map = pd.Series(nodes['id'].values, index = nodes['label']).to_dict()\n",
    "    links = links.replace({'from': label_id_map})\n",
    "    links = links.replace({'to': label_id_map})\n",
    "\n",
    "    return (links, nodes)\n",
    "\n",
    "# Create links and nodes.\n",
    "links, nodes = create_nodes_and_links(data)\n",
    "\n",
    "# Save data.\n",
    "links.to_csv(\"../main/links.tsv\", sep = r\"\\t\", index = False)\n",
    "nodes.to_csv(\"../main/nodes.tsv\", sep = r\"\\t\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
